---
title: "Homework 2"
output: github_document
---
_Note: This homework uses the `tidyverse`, `dplyr`, and `readxl` libraries._
```{r setup, include = FALSE}
library(tidyverse)
library(dplyr)
library(readxl)
```

## Problem 1

```{r data, include = FALSE}
nyctransit_df = read_csv("./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv", na = c("NA", "", "."))
nyctransit_df #1,868 entries and 32 columns.
```

The `nyctransit_df` dataset (`r ncol(nyctransit_df)` columns x `r nrow(nyctransit_df)` rows) was created based on the _"NYC_Transit_Subway_Entrance_And_Exit_Data.csv"_ file.

```{r summary, include = FALSE}
summary(nyctransit_df)
nyctransit_df = janitor::clean_names(nyctransit_df) #transform variable names to snake case
```

After reading the `nyctransit_df` dataset and taking a look at its 32 variables with `summary(nyctransit_df)`, I applied the snake case convention to the variable names using `clean_names` from the `janitor` package. 

```{r select, include = FALSE}
nyctransit_q1 = 
  select(nyctransit_df, line:entry, vending, ada) |>
   mutate(
     entry = ifelse(entry == "YES", TRUE, FALSE)
     ) #convert entry to logical variable

nyctransit_q1 #1,868 entries and 19 columns. 11 character, 2 logical and 6 double variables
summary(nyctransit_q1)
```

Using the `select()` function, I selected my variables of interest and converted the `entry` variable in this dataset from character to a logical variable. 

The resulting `nyctransit_q1` dataset (`r ncol(nyctransit_q1)` columns x `r nrow(nyctransit_q1)` rows) has 19 variables (11 characters, 2 logicals, and 6 doubles).


#### The 19 Variables in `nyctransit_q1`

_Note: To see the categories in some of the character and double variables, I transformed them into factors and created the `nyctransit_fc` dataset. Then I used the `levels(pull())` function to look at the categories._

```{r char to fact, include = FALSE}
nyctransit_fc = 
  nyctransit_q1 |>
    mutate(
      line_fc = as.factor(line), #mutate to see the categories
      station_name_fc = as.factor(station_name),
      route1_fc = as.factor(route1),
      route2_fc = as.factor(route2),
      route3_fc = as.factor(route3),
      route4_fc = as.factor(route4),
      route5_fc = as.factor(route5),
      route6_fc = as.factor(route6),
      route7_fc = as.factor(route7),
      route8_fc = as.factor(route8), # trains 1,4, and 5 
      route9_fc = as.factor(route9), # trains 2 and 5
      route10_fc = as.factor(route10), # train 3
      route11_fc = as.factor(route11), # train 7
      entrance_type_fc = as.factor(entrance_type),
      vending_fc = as.factor(vending),
      )

summary(nyctransit_fc)
```

```{r levels, include = FALSE}
levels(pull(nyctransit_fc,line_fc))
levels(pull(nyctransit_fc,station_name_fc))
levels(pull(nyctransit_fc,route1_fc))
levels(pull(nyctransit_fc,route2_fc))
levels(pull(nyctransit_fc,route3_fc))
levels(pull(nyctransit_fc,route4_fc))
levels(pull(nyctransit_fc,route5_fc))
levels(pull(nyctransit_fc,route6_fc))
levels(pull(nyctransit_fc,route7_fc))
levels(pull(nyctransit_fc,entrance_type_fc))
levels(pull(nyctransit_fc,vending_fc))
```

In the `nyvtransit_q1` dataset, the 11 character variables include: `line` (36 different lines), `station_name` (356 different names), `route1` to `route7`, `entrance_type` (`r levels(pull(nyctransit_fc,entrance_type_fc))`), and `vending` (y/n).

The 2 logical variables are `ada`, which indicates whether it is ADA compliant, and `entry`.

The 6 double variables are `station_latitude`, `station_longitude`, and `route8` to `route11`.
  
The data in `nyctransit_q1` is not tidy. For example, Routes 8 to 11 were entered as doubles but it is a categorical variable. Similarly, character variables such as `line`, `entrance_type`, `vending`, and `route1` to `route7` are categorical variables and could be converted to factors.

#### Distinct Stations and ADA Compliance

```{r station, include = FALSE}
levels(pull(nyctransit_fc,station_name_fc)) #can also use distinct function

station = distinct(nyctransit_fc,station_name) #another option: 356 stations
```

There are 356 distinct stations. 73 out of the 356 stations are ADA compliant. Of the 36 distinct stations that have entrances/exits without vending, 16 (44.44%) allow entrance. 

```{r ada stations, include = FALSE}
nyctransit_ada = filter(nyctransit_fc, ada == "TRUE")
ada = distinct(nyctransit_ada,station_name) #73 distinct stations
```

```{r vending, include = FALSE}
nyctransit_ent = filter(nyctransit_ada, vending_fc == "NO") #36 stations with entrance/exits w/o vending
summary(nyctransit_ent) #Entry: 20 FALSE and 16 TRUE
```


```{r reformat, include = FALSE}
reformat = 
  nyctransit_fc |>
    mutate(
      route8 = as.character(route8_fc), 
      route9 = as.factor(route9_fc),
      route10 = as.factor(route10_fc),
      route11 = as.factor(route11_fc)) |>
    pivot_longer(
    cols = route1:route11, 
    names_to = "route_number",
    names_prefix = "route_number",
    values_to = "train") |>
    filter(train == "A"
  ) 
```

```{r trainA stations, include = FALSE}
trainA_ada = distinct(reformat, station_name_fc)
```

After reformatting the data, we find that 56 distinct stations serve the A train. Of the stations that serve the A train, 16 are ADA compliant.

```{r trainA ada, include = FALSE}
ada_compli = reformat |>
    filter(ada == TRUE) |>
    distinct(station_name)

ada_compli #16 distinct ADA compliant stations serve A train
```


## Problem 2

Problem 2 uses the _"202409 Trash Wheel Collection Data"_ xlsx file from Mr. Trash Wheel. 

```{r mrtrash setup, include = FALSE}
mr_trash = read_excel("data/202409 Trash Wheel Collection Data.xlsx", na = c("NA", "", "."), sheet = "Mr. Trash Wheel", n_max = 651) |>
janitor::clean_names() |>
  select(dumpster:homes_powered) |> 
  mutate(
    sports_balls = as.integer(sports_balls),
    year = as.factor(year)) |>
  add_column(trash_wheel_name = "Mr")
  
mr_trash #651 x 15
```

```{r proftrash setup, include = FALSE}
prof_trash = read_excel("data/202409 Trash Wheel Collection Data.xlsx", na = c("NA", "", "."), sheet = "Professor Trash Wheel", n_max = 118) |>
janitor::clean_names() |>
add_column(trash_wheel_name = "Prof") |>
  mutate(year = as.factor(year))

prof_trash #118 x 14
```

```{r gwynndatrash setup, include = FALSE}
gwynnda_trash = 
  read_excel("data/202409 Trash Wheel Collection Data.xlsx", na = c("NA", "", "."), sheet = "Gwynnda Trash Wheel", n_max = 262) |>
  janitor::clean_names() |>
  add_column(trash_wheel_name = "Gwynnda") |>
  mutate(year = as.factor(year))

gwynnda_trash #262 x 13
```

```{r merge1, include = FALSE}
Combined_Trash = 
  bind_rows(mr_trash, prof_trash, gwynnda_trash) 

Combined_Trash #1,031 x 15. Rows make sense
```

#### Summary of Data
3 sheets from the xlsx file (Mr. Trash Wheel, Professor Trash Wheel, Gwynnda Trash Wheel) have been read, cleaned with the `clean_names` function, and imported. 
An additional variable `trash_wheel_name` was created for all three datasets: 

* `mr. trash` (`r nrow(mr_trash)` x `r ncol(mr_trash)`)
* `prof_trash` (`r nrow(prof_trash)` x `r ncol(prof_trash)`)
* `gwynnda_trash` (`r nrow(gwynnda_trash)` x `r ncol(gwynnda_trash)`)

`year` was standardized to a factor variable in all three to allow later merging of the datasets. Entries in the `sports_ball` variable has been converted to integers for `mr. trash` only. 

After merging, the resulting dataset `Combined_Trash` has `r nrow(Combined_Trash)` observations and `r ncol(Combined_Trash)` variables. Examples of key variables include:

* `trash_wheel_name` lists the specific names of the trash wheels
* `dumpster` the dumpster number
*  Date is specified by the `year`, `month`, and `date` variables
* `weight_tons` and `volume_cubic_yards` of collected trash
* Trash categories include `plastic_bottles`, `polystyrene`, `cigarette_butts`, `glass_bottles`, `plastic_bags`, `wrappers`, `sports_balls`
* `homes_powered`

```{r weight and cigb, include = FALSE}
trashweight_prof = 
  prof_trash |>
  select(weight_tons)

cigbutts_gwyn = 
  gwynnda_trash |>
  select(cigarette_butts, month, year) |>
  filter(year == 2022, month == "June") |>
  select(cigarette_butts)

sum(cigbutts_gwyn)
```

The total weight of trash collected by Professor Trash Wheel was `r sum(trashweight_prof)` tons.

In June of 2022, Gwynnda collected a total of 18,120 cigarette butts.

## Problem 3
Problem 3 uses 3 datasets from the Great British Bake Off: _"bakers.csv", "bakes.csv",_ and _"results.csv"_.

#### Part 1
```{r bakers data, include = FALSE}
bakers_df = read_csv(file = "./data/gbb/bakers.csv", na = c("NA", "", ".")) |> #120x5
  janitor::clean_names() |>
  separate(baker_name, into = c("baker", "baker_ln"), sep = " ")

#Converted series into factor variables since it is a categorical variable.
summary(bakers_df)
```

```{r bakes data, include = FALSE}
bakes_df = read_csv(file = "./data/gbb/bakes.csv", na = c("NA", "", ".")) |> #548x5
  janitor::clean_names()

#Converted series and episode into factor variables since it is a categorical variable.

summary(bakes_df)
```

```{r results data, include = FALSE}
results_df = read_csv(file = "./data/gbb/results.csv", na = c("NA", "", "."), skip = 2) |>
  janitor::clean_names()

summary(results_df) #1,136 x 5

```

##### Summary of Data
The 3 files have been read, cleaned with the `clean_names` function, and imported as `bakers_df` (120 x 5), `bakes_df` (548 x 5), and `results_df` (1,136 x 5) datasets. `results_df` was imported by skipping the first two rows so the dataset makes more sense. After looking at the variables, I standardized the `series` and `episode` variables to factors since they are categorical variables. I also separated the `baker_names` variable into two columns, one of which is named `baker`. This allows easier merging of the datasets later on.

**Variables**

* `bakers_df` has 3 character variables (`baker_name`, `baker_occupation`, `hometown`) 1 factor (`series`), and 1 double (`baker_age`).
* `bakes_df` has 3 character variables (`baker`, `signature_bake`,`show_stopper`) and 2 factors (`episode`,`series`).
* `results_df` has 2 character variables (`baker`, `results`), 2 factors (`series`, `episode`) and 1 double (`technical`)

##### Comparing datasets using `anti_join`

```{r antijoin1, include = FALSE}
bakers_bakes = anti_join(bakers_df, bakes_df)

bakes_df = bakes_df |>
  mutate(baker = gsub('"Jo"','Jo', baker)) #updated Jo

bakes_bakers = anti_join(bakes_df, bakers_df) #none
```

Using the `anti_join` function, I compared the datasets with each other.

* `bakers_bakes` (joined by `baker` and `series`) was created to compare data between `bakers_df` and `bakes_df`. It shows that 26 bakers in `bakers_df` were absent in `bakes_df`, including Jo Wheatley. However, she is actually included in the `bakes_df` with her name in quotation marks. Thus, I updated her name in the `bakes_df` without the quotations.

```{r antijoin2, include = FALSE}
bakes_results = anti_join(bakes_df, results_df)

results_df = results_df |>
  mutate(baker = gsub('Joanne','Jo', baker)) #updated Jo

results_bakes = anti_join(results_df, bakes_df) #none
```

* `bakers_results` (joined by `baker`, `series`, and `episodde`) shows that Jo was present in `bakers_df` but absent in `results_df`. This is because she is entered as "Joanne" for the second dataset. Thus, I updated her name to "Jo" in `results_df`.

```{r antijoin3, include = FALSE}

results_bakers = anti_join(results_df, bakers_df) #none

bakers_results = anti_join(bakes_df, results_df) #none
```

* After the above updates, no entries were listed in `results_bakers` (joined by `baker` and `series`), which compares `results_df` and `bakers_df`.

##### Merging 

```{r merge_gbb, include = FALSE}
first_merge =
  left_join(bakes_df, bakers_df, by = c("series", "baker")) 

combined_data = 
  left_join(results_df, first_merge, by = c("series", "episode", "baker")) |>
  select(series, episode, baker, baker_ln, baker_age, baker_occupation, hometown, signature_bake, show_stopper, technical, result) |>
  drop_na("result") |> #delete entries of bakers that were dropped
  rename(baker_first_name = baker, baker_last_name = baker_ln
 ) 
  
summary(combined_data)
```

```{r csv, include = FALSE}
write_csv(combined_data, "./data/gbb/combined_data.csv")
```

All three datasets have been merged to `combined_data` and columns rearranged. Entries without values in `results` were deleted and the file was exported as _"combined_data.csv"_ to the data folder of this repository.The dataset contains information on participants (names, age, occupation, and hometown), their  baking creations (`signature_bake`, `show_stopper`), `technical` scores, and their `result` at the end of each episode. There is a total of 10 seasons with 10 episodes each. `combined_data` has `r ncol(combined_data)` variables and `r nrow(combined_data)` observations.


**Key for `results` in `results_df` dataset**

* IN = stayed in
* OUT = Eliminated
* STAR BAKER = Star Baker
* WINNER = Series Winner
* Runner-up = Series Runner up
* WD = withdrew


#### Part 2

```{r}
winner_starbaker = combined_data |>
  select(series:baker_first_name, technical, result) |>
  mutate(result = as.factor(result)) |>
  filter(series > 4 & (result == "STAR BAKER" | result == "WINNER"))

summary(winner_starbaker) #54 star bakers and 6 winners

bakers_5_to_10 = distinct(winner_starbaker,baker_first_name) #32 bakers

star_bakers = winner_starbaker |>
  filter(result == "STAR BAKER")

summary(combined_data) #1-13 for technical

summary(star_bakers) #1 - 19 star bakers; range: 1-8
```

There are 54 Star Bakers and 6 Winners out of the 32 bakers who participated in Series 5 to 10. After looking at the data, I noticed that the winners in these series had either 1 or 2 for `technical`. The star bakers had a range of 1 to 8 for `technical` with 19/54 (35.19%) in 1. The range for the values in `technical` in the `combined_data` is from 1 to 13. Bakers with a smaller value for `technical` may have a greater chance of becoming star bakers.


