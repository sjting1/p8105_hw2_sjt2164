Homework 2
================

*Note: This homework uses the `tidyverse`, `dplyr`, and `readxl`
libraries.*

## Problem 1

The `nyctransit_df` dataset (32 columns x 1868 rows) was created based
on the *“NYC_Transit_Subway_Entrance_And_Exit_Data.csv”* file.

After reading the `nyctransit_df` dataset and taking a look at its 32
variables with `summary(nyctransit_df)`, I applied the snake case
convention to the variable names using `clean_names` from the `janitor`
package.

Using the `select()` function, I selected my variables of interest and
converted the `entry` variable in this dataset from character to a
logical variable.

The resulting `nyctransit_q1` dataset (19 columns x 1868 rows) has 19
variables (11 characters, 2 logicals, and 6 doubles).

#### The 19 Variables in `nyctransit_q1`

*Note: To see the categories in some of the character and double
variables, I transformed them into factors and created the
`nyctransit_fc` dataset. Then I used the `levels(pull())` function to
look at the categories.*

In the `nyvtransit_q1` dataset, the 11 character variables include:
`line` (36 different lines), `station_name` (356 different names),
`route1` to `route7`, `entrance_type` (Door, Easement, Elevator,
Escalator, Ramp, Stair, Walkway), and `vending` (y/n).

The 2 logical variables are `ada`, which indicates whether it is ADA
compliant, and `entry`.

The 6 double variables are `station_latitude`, `station_longitude`, and
`route8` to `route11`.

The data in `nyctransit_q1` is not tidy. For example, Routes 8 to 11
were entered as doubles but it is a categorical variable. Similarly,
character variables such as `line`, `entrance_type`, `vending`, and
`route1` to `route7` are categorical variables and could be converted to
factors.

#### Problem 1 Question Answers

There are 356 distinct stations. 73 out of the 356 stations are ADA
compliant. Of the 36 distinct stations that have entrances/exits without
vending, 16 (44.44%) allow entrance.

#### Reformatting Data

After reformatting the data, we find that 56 distinct stations serve the
A train.

Of the stations that serve the A train, 16 are ADA compliant.

## Problem 2

Problem 2 uses the *“202409 Trash Wheel Collection Data”* xlsx file from
Mr. Trash Wheel.

#### Summary of Data

3 sheets from the xlsx file (Mr. Trash Wheel, Professor Trash Wheel,
Gwynnda Trash Wheel) have been read, cleaned with the `clean_names`
function, and imported. An additional variable `trash_wheel_name` was
created for all three datasets:

- `mr. trash` (651 x 15),
- `prof_trash` (118 x 14), and  
- `gwynnda_trash` (262 x 13).

`year` was standardized to a factor variable in all three to allow later
merging of the datasets. Entries in the `sports_ball` variable has been
converted to integers for `mr. trash` only.

After merging, the resulting dataset `Combined_Trash` has 1031
observations and 15 variables. Examples of key variables include:

- `trash_wheel_name` lists the specific names of the trash wheels
- `dumpster` the dumpster number
- Date is specified by the `year`, `month`, and `date` variables
- `weight_tons` and `volume_cubic_yards` of collected trash
- Trash categories include `plastic_bottles`, `polystyrene`,
  `cigarette_butts`, `glass_bottles`, `plastic_bags`, `wrappers`,
  `sports_balls`
- `homes_powered`

The total weight of trash collected by Professor Trash Wheel was 246.74
tons.

In June of 2022, Gwynnda collected a total of 1,8120 cigarette butts.

## Problem 3

Problem 3 uses 3 datasets from the Great British Bake Off:
*“bakers.csv”, “bakes.csv”,* and *“results.csv”*.

#### Part 1

#### Summary of Data

The 3 files have been read, cleaned with the `clean_names` function, and
imported as `bakers_df` (120 x 5), `bakes_df` (548 x 5), and
`results_df` (1,136 x 5) datasets. `results_df` was imported by skipping
the first two rows so the dataset makes more sense. After looking at the
variables, I standardized the `series` and `episode` variables to
factors since they are categorical variables. I also separated the
`baker_names` variable into two columns, one of which is named `baker`.
This allows easier merging of the datasets later on.

**Variables**

- `bakers_df` has 3 character variables (`baker_name`,
  `baker_occupation`, `hometown`) 1 factor (`series`), and 1 double
  (`baker_age`).
- `bakes_df` has 3 character variables (`baker`,
  `signature_bake`,`show_stopper`) and 2 factors (`episode`,`series`).
- `results_df` has 2 character variables (`baker`, `results`), 2 factors
  (`series`, `episode`) and 1 double (`technical`)

**Key for `results` in `results_df` dataset**

- IN = stayed in
- OUT = Eliminated
- STAR BAKER = Star Baker
- WINNER = Series Winner
- Runner-up = Series Runner up
- WD = withdrew

### Comparing datasets using `anti_join`

Using the `anti_join` function and joining by `baker` and `series`, I
compared the datasets with each other.

- `bakers_bakes` was created to compare data between `bakers_df` and
  `bakes_df`. It shows that 26 bakers in `bakers_df` were absent in
  `bakes_df`, including Jo Wheatley. However, she is actually included
  in the `bakes_df` with her name in quotation marks. Thus, I updated
  her name in the `bakes_df` without the quotations.

- `bakers_results` shows that Jo was present in `bakers_df` but absent
  in `results_df`. This is because she is entered as “Joanne” for the
  second dataset. Thus, I updated her name to “Jo” in `results_df`.

- After the above updates, no entries were listed in `results_bakers`,
  which compares `results_df` and `bakers_df`.

### Merging

All three datasets have been merged to `combined_data` and columns
rearranged. Entries without values in `results` were deleted and the
file was exported as *“combined_data.csv”* to the data folder of this
repository.The dataset contains information on participants (names, age,
occupation, and hometown), their baking creations (`signature_bake`,
`show_stopper`), `technical` scores, and their `result` at the end of
each episode. There is a total of 10 seasons with 10 episodes each.
`combined_data` has 11 variables and 710 observations.
